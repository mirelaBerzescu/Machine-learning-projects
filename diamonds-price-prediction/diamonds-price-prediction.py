# -*- coding: utf-8 -*-
"""mirela-MiniProiect_Regresie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J2rPNXJ0b8wRzqH-lg_-AGA9Xt46VLTJ

# ğŸ§© Mini-Proiect: Regresie È™i ÃmbunÄƒtÄƒÈ›ire de Model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv"
pd.set_option("display.max_columns", 100)  # aratÄƒ pÃ¢nÄƒ la 100 de coloane
pd.set_option("display.width", 100)        # lÄƒÈ›imea maximÄƒ a liniei
df = pd.read_csv(url)
df.head(15)

print(df.isnull())

print("Valori lipsÄƒ pe coloanÄƒ:")
print(df.isnull().sum())

df.info()

df["carat"].value_counts()
df["clarity"].value_counts()
df["cut"].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

num_cols = ["carat","depth","table","x","y","z","price"]
corr_pearson = df[num_cols].corr(method="pearson") # Se calculeazÄƒ corelaÈ›ia
plt.figure(figsize=(6,4))
sns.heatmap(corr_pearson, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
plt.title("CorelaÈ›ie (Pearson) â€” variabile numerice")
plt.tight_layout()
plt.show()


plt.show()

print(df.groupby("cut")["price"].mean().sort_values(ascending=False))
print(df.groupby("cut")["price"].median().sort_values(ascending=False))


sns.violinplot(data=df, x="cut", y="price", inner="quartile")
plt.title("Violinplot â€” Pret per cut")
plt.xlabel("cut"); plt.ylabel("Price")

# ğŸ”¹ 3ï¸âƒ£ ConstruiÈ›i un model iniÈ›ial

# Tipuri posibile:
# - Linear Regression
# - Multiple Regression
# - Polynomial Regression

feature_cols = ["carat"]
target_col = "price"
x = df[feature_cols].values
y = df[target_col].values

X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred_test = model.predict(X_test)

print("=== Linear Regression (univariatÄƒ) ===")
print(f"Coeficient (slope):  {model.coef_[0]:.6f}")
print(f"Intercept (bias):    {model.intercept_:.6f}")

# 7) Vizualizare: punctele + linia de regresie
xs = np.linspace(x.min(), x.max(), 200).reshape(-1, 1)  # grid pe axa X
ys = model.predict(xs)                                  # linia estimatÄƒ

plt.figure(figsize=(6,4))
plt.scatter(x, y, s=15, alpha=0.7, label='date')
plt.plot(xs, ys, linewidth=3, label='fit linear')
plt.xlabel('carat')
plt.ylabel('price')
plt.title('Linear Regression (univariatÄƒ)')
plt.legend()
plt.grid(True)
plt.show()

# ğŸ”¹ 4ï¸âƒ£ EvaluaÈ›i performanÈ›a modelului


# CalculaÈ›i urmÄƒtoarele metrici:
# - RMSE (Root Mean Squared Error)
# - RÂ² (coeficientul de determinare)

rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
print("Rmse:", rmse)
r2 = r2_score(y_test, y_pred_test)

print("RÂ²:", r2)

# ğŸ”¹ 5ï¸âƒ£ ÃmbunÄƒtÄƒÈ›iÈ›i modelul


# ÃncercaÈ›i una sau mai multe dintre:
# - Normalizare / Standardizare
# - Regularizare (Ridge / Lasso)
# - Grad polinomial diferit
# - Schimbarea unor feature-uri
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, Lasso

feature_cols = ["carat","x","y","z"]
target_col = "price"

X = df[feature_cols].values
y = df[target_col].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3) AntrenÄƒm modelul de regresie liniarÄƒ
model = LinearRegression()
model.fit(X_train, y_train)

# 4) Facem predicÈ›ii pe test
y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Rmse multiple features:", rmse)  #1521.8093468210932 (mai mic)
r2 = r2_score(y_test, y_pred)  #0.854316556603928 ( pare ca r2 s-a mai imbunatatit dar foarte putin )

print("RÂ² multiple features:", r2)

#1.Standardizare

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LinearRegression()
model.fit(X_train_scaled, y_train)

y_pred_stand = model.predict(X_test_scaled)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_stand))
r2 = r2_score(y_test, y_pred_stand)

print("Rmse dupa standardizare :", rmse)
print("RÂ² dupa standardizare :", r2)

from sklearn.linear_model import LassoCV

alphas = [0.0001, 0.001,0.003,0.00003, 0.01, 0.1, 1, 10,20,30, 100]
lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=300000)
lasso_cv.fit(X_train_scaled, y_train)

print("Cel mai bun alpha:", lasso_cv.alpha_)

# Lasso Regression
lasso_model = Lasso(alpha=10)
lasso_model.fit(X_train_scaled, y_train)
y_pred_lasso = lasso_model.predict(X_test_scaled)

r2 = r2_score(y_test, y_pred_lasso)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))

print("Rmse after laso:", rmse)
print("RÂ² after lasso:", r2)

# 2) Construim feature-uri polinomiale (include_bias=False ca sÄƒ nu mai adÄƒugÄƒm manual coloana de 1)
degree = 2

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
target_col = 'price'
feature_cols = [c for c in num_cols if c != target_col]

print(feature_cols)
X = df[feature_cols].values
y = df[target_col].values

# 1) Split first
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 2) Polynomial features
poly = PolynomialFeatures(degree=degree, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# 3) Fit model
model = LinearRegression()
model.fit(X_train_poly, y_train)


# 5) Prezicem pe test È™i calculÄƒm MSE (o singurÄƒ metricÄƒ, simplu)
y_pred_poly = model.predict(X_test_poly)

r2 = r2_score(y_test, y_pred_poly)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_poly))

print("Rmse after poly:", rmse)
print("RÂ² after poly:", r2)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

df_encoded = pd.get_dummies(
    df,
    columns=['cut', 'color', 'clarity'],
    drop_first=True
)

# Features & target
X = df_encoded.drop(columns='price')
y = df_encoded['price']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# AntrenÄƒm modelul GradientBoosting
gb_model = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

gb_model.fit(X_train, y_train)

y_pred = gb_model.predict(X_test)

# EvaluÄƒm
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("RÂ²:", r2_score(y_test, y_pred))

from xgboost import XGBRegressor

xgb = XGBRegressor(
    objective="reg:squarederror",
    random_state=42,
    n_jobs=-1
)

xgb.fit(X_train, y_train)

y_pred = xgb.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("RÂ²:", r2)

from sklearn.model_selection import GridSearchCV
param_grid = {
    "n_estimators": [100,200, 400],
    "learning_rate": [0.05, 0.1],
    "max_depth": [4, 6, 8,10],
    "subsample": [0.8, 1.0],
    "colsample_bytree": [0.8, 1.0]
}

grid = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring="neg_root_mean_squared_error",  # regression metric
    cv=5,
    verbose=1,
    n_jobs=-1
)

grid.fit(X_train, y_train)

print("Best params:", grid.best_params_)
print("Best CV RMSE:", -grid.best_score_)

best_xgb = grid.best_estimator_

y_pred = best_xgb.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("Test RMSE:", rmse)
print("Test RÂ²:", r2)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score,ConfusionMatrixDisplay

url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv"
df = pd.read_csv(url)
# One-Hot Encoding
df_encoded = pd.get_dummies(
    df,
    columns=['cut', 'color', 'clarity'],
    drop_first=True
)

# Features & target
X = df_encoded.drop(columns='price')
y = df_encoded['price']

# Train / test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Model
xgb = XGBRegressor(
    objective="reg:squarederror",
    n_estimators=200,
    learning_rate=0.05,
    reg_alpha=4,   # L1
    reg_lambda=4,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)

# Train
xgb.fit(X_train, y_train)

# Predict
y_pred = xgb.predict(X_test)

# Metrics
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("RÂ²:", r2)